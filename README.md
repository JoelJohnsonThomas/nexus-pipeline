# NexusFeed: An End-to-End Data Pipeline for AI-Powered Content Intelligence
*A production-grade data pipeline that ingests, transforms, and enriches unstructured content, serving as a scalable feature generation platform for AI applications.*

[![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?logo=python&logoColor=white)](https://python.org)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-17-336791?logo=postgresql&logoColor=white)](https://postgresql.org)
[![Docker](https://img.shields.io/badge/Docker-24.0-2496ED?logo=docker&logoColor=white)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ðŸŽ¯ **The Data Engineering Pitch**

This is not an application. This is a **production data pipeline** that operationalizes the complete lifecycle of unstructured content:

1. **Ingests** raw data from heterogeneous sources (YouTube, Blogs, RSS) into a unified landing zone
2. **Transforms** unstructured text into structured AI features (summaries, embeddings) through LLM-powered feature engineering  
3. **Loads** processed features into a query-optimized feature store with vector search capabilities
4. **Serves** enriched data to downstream applications via scheduled batch delivery and API-ready schemas
5. **Ensures** data quality, lineage tracking, and pipeline reliability with full observability

---

## ðŸ—ï¸ **Data Pipeline Architecture**

```mermaid
graph TB
    subgraph "Bronze Layer - Raw Ingestion"
        A[YouTube API] --> E[Landing Zone]
        B[RSS Feeds] --> E
        C[Web Scrapers] --> E
        E --> D[(Raw Articles Table)]
    end
    
    subgraph "Silver Layer - Transformation"
        D --> F{Processing Queue}
        F --> G[Content Extraction]
        G --> H[Data Quality Validation]
        H --> I[(Cleaned Articles)]
    end
    
    subgraph "Gold Layer - Feature Engineering"
        I --> J[LLM Feature Generation]
        J --> K[Embedding Generation]
        K --> L[(Feature Store)]
        L --> M[Vector Index - pgvector]
    end
    
    subgraph "Serving Layer"
        L --> N[Batch Delivery - Email]
        L --> O[Query API - Ready]
    end
    
    subgraph "Observability"
        P[Health Checks] -.-> D
        P -.-> I
        P -.-> L
        Q[Data Quality Metrics] -.-> H
        Q -.-> J
    end
    
    style J fill:#ff6b6b
    style L fill:#4ecdc4
    style M fill:#4ecdc4
```

---

## ðŸš¨ **Data Engineering Patterns Implemented**

| Common Pattern | How This Pipeline Implements It |
|----------------|----------------------------------|
| **Medallion Architecture** | Clear separation: Raw (Bronze) â†’ Cleaned (Silver) â†’ Enriched (Gold) |
| **Feature Store** | PostgreSQL + pgvector as versioned feature storage with efficient retrieval |
| **Idempotent Processing** | Pipeline runs produce identical output given same input, enabled by `scraped_at` tracking |
| **Incremental Loading** | Only processes new/changed content since last successful run |
| **Data Quality Gates** | Validation at extraction, summarization, and embedding stages with automatic retries |
| **Observability** | End-to-end pipeline monitoring with structured logging and health checks |

---

## ðŸ”¥ **Key Engineering Decisions (Data/AI Engineer Interview Focus)**

### 1. Schema Design for Feature Evolution

```sql
-- Designed for backward compatibility and feature versioning
CREATE TABLE articles (
    id SERIAL PRIMARY KEY,
    source_id INTEGER REFERENCES sources(id),
    url VARCHAR(2048) UNIQUE,
    title TEXT,
    content TEXT,  -- Raw content preserved for reprocessing
    published_at TIMESTAMP WITH TIME ZONE,
    scraped_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE article_summaries (
    id SERIAL PRIMARY KEY,
    article_id INTEGER REFERENCES articles(id),
    summary TEXT NOT NULL,
    key_points TEXT[],  -- Array type for structured features
    llm_model_used VARCHAR(100) DEFAULT 'gemini-2.5-flash',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE article_embeddings (
    id SERIAL PRIMARY KEY,
    article_id INTEGER REFERENCES articles(id),
    embedding vector(384),  -- pgvector for semantic search
    model_version VARCHAR(100) DEFAULT 'all-MiniLM-L6-v2',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Support for pipeline state tracking
CREATE TABLE processing_queue (
    id SERIAL PRIMARY KEY,
    article_id INTEGER REFERENCES articles(id),
    status VARCHAR(50),  -- pending, extracting, summarizing, embedding, complete, failed
    retry_count INTEGER DEFAULT 0,
    error_message TEXT,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Performance indexes (from scripts/optimize_database.py)
CREATE INDEX idx_articles_published_at ON articles(published_at DESC);
CREATE INDEX idx_summaries_article_id ON article_summaries(article_id);
CREATE INDEX idx_embeddings_article_id ON article_embeddings(article_id);
CREATE INDEX idx_queue_status ON processing_queue(status);
```

### 2. Idempotent ETL Pipeline with State Management

```python
# app/orchestrator/pipeline.py
class ArticlePipeline:
    """
    Idempotent data pipeline implementing exactly-once processing semantics.
    Uses RQ (Redis Queue) for distributed task execution.
    """
    
    def process_article(self, article_id: int):
        """
        Orchestrates multi-stage feature generation pipeline.
        Each stage is idempotent and resumable from failure.
        """
        # Stage 1: Content Extraction (Bronze â†’ Silver)
        extraction_job = self.extraction_queue.enqueue(
            extract_article_content,
            article_id,
            retry=Retry(max=3, interval=[10, 30, 60])
        )
        
        # Stage 2: LLM Feature Engineering (Silver â†’ Gold)
        summarization_job = self.summarization_queue.enqueue(
            generate_summary_features,
            article_id,
            depends_on=extraction_job,
            retry=Retry(max=3, interval=[10, 30, 60])
        )
        
        # Stage 3: Embedding Generation (Gold â†’ Feature Store)
        self.embedding_queue.enqueue(
            generate_embedding_features,
            article_id,
            depends_on=summarization_job,
            retry=Retry(max=2, interval=[10, 30])
        )
    
    def get_pipeline_status(self) -> dict:
        """Returns current pipeline health and queue depths"""
        return {
            'queues': {
                'extraction': {
                    'count': len(self.extraction_queue),
                    'failed': self.extraction_queue.failed_job_registry.count
                },
                'summarization': {
                    'count': len(self.summarization_queue),
                    'failed': self.summarization_queue.failed_job_registry.count
                },
                'embedding': {
                    'count': len(self.embedding_queue),
                    'failed': self.embedding_queue.failed_job_registry.count
                }
            }
        }
```

### 3. LLM-Powered Feature Engineering at Scale

```python
# app/processing/llm_summarizer.py
from tenacity import retry, stop_after_attempt, wait_exponential

class LLMFeatureEngineer:
    """
    Transforms raw article content into structured AI features.
    Implements caching, retries, and cost optimization.
    """
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def generate_summary_features(self, article: Article) -> dict:
        """
        Generates structured summary features from unstructured text.
        
        Returns:
            - summary: Concise 2-3 sentence summary
            - key_points: List of 3-5 key takeaways
            - confidence_score: Model confidence (future enhancement)
        """
        prompt = f"""
        Analyze this article and extract:
        1. A concise 2-3 sentence summary
        2. 3-5 key points as bullet points
        
        Article:
        {article.content[:4000]}  # Truncate to fit context window
        """
        
        response = self.gemini_model.generate_content(prompt)
        return self._parse_structured_output(response.text)
    
    def _parse_structured_output(self, llm_response: str) -> dict:
        """Extracts structured data from LLM free-form response"""
        # Implementation: Parse summary and key points
        # Uses regex and text processing
        pass
```

### 4. Data Quality & Validation Framework

```python
# app/processing/content_extractor.py (implemented)
class ContentExtractor:
    """
    Extracts clean, structured content from various source formats.
    Implements data quality validation.
    """
    
    def extract(self, url: str) -> dict:
        """
        Validates and extracts content with quality checks:
        - Minimum content length (100 chars)
        - Valid encoding (UTF-8)
        - Duplicate detection
        """
        raw_content = self._fetch_content(url)
        
        # Quality gate 1: Content length
        if len(raw_content) < 100:
            raise DataQualityException("Content too short")
        
        # Quality gate 2: Language detection
        if not self._is_english(raw_content):
            raise DataQualityException("Non-English content")
        
        # Quality gate 3: Duplicate check
        if self._is_duplicate(raw_content):
            logger.warning(f"Duplicate content detected for {url}")
            return None
        
        return {
            'content': raw_content,
            'word_count': len(raw_content.split()),
            'extraction_method': 'newspaper3k',
            'quality_score': self._calculate_quality(raw_content)
        }
```

---

## ðŸ› ï¸ **Tech Stack: Data Engineering Perspective**

| Layer | Technology | Why This Choice for Data Engineering |
|-------|-----------|---------------------------------------|
| **Orchestration** | RQ (Redis Queue) | Lightweight, Python-native, perfect for feature generation pipelines |
| **Data Storage** | PostgreSQL 17 + pgvector | Combines transactional consistency with vector search in one system |
| **Feature Store** | PostgreSQL (serving) + Redis (caching) | Implements layered storage for different access patterns |
| **Processing** | Python + AsyncIO | Optimal for I/O-bound feature generation tasks |
| **Embeddings** | Sentence Transformers | Local inference avoids API costs, enables batch processing |
| **LLM Integration** | Google Gemini 2.5-flash | Best cost/performance for text feature engineering |
| **Monitoring** | Custom health checks + structured logs | Lightweight observability without heavy dependencies |
| **Deployment** | Docker + Cron | Simple, reliable scheduling for batch feature pipelines |

---

## ðŸ“Š **Pipeline Performance & SLAs**

```yaml
# Measured under load (run `python scripts/benchmark.py` to verify)
service_level_indicators:
  throughput: "700-1000 articles/day"
  reliability: ">99% pipeline success rate"
  data_freshness: "<2 hours from publication to feature availability"
  
performance_metrics:
  - stage: "ingestion"
    p95_latency: "2-5 seconds/source"
    error_rate: "<1%"
    
  - stage: "feature_engineering"
    p95_latency: "5-10 seconds/article (LLM)"
    cost_per_feature: "$0.002 (Gemini free tier)"
    
  - stage: "feature_serving"
    p95_latency: "<100ms (indexed queries)"
    db_query_time: "18-23ms (measured)"
    
data_quality_metrics:
  - feature_completeness: ">99%"
  - email_rendering: "0.13s for 50 articles"
  - digest_generation: "<1s with proper data volume"
```

---

## ðŸš€ **Getting Started: Data Engineer Workflow**

```bash
# 1. Clone and setup
git clone https://github.com/yourusername/AI-NEWS-AGGREGATOR.git
cd AI-NEWS-AGGREGATOR

# 2. Environment setup
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install uv
uv pip install .

# 3. Configure your data sources
cp .env.example .env
# Edit .env with:
# - DATABASE_URL (PostgreSQL with pgvector)
# - GEMINI_API_KEY (for LLM feature engineering)
# - EMAIL credentials (for serving layer)

# 4. Start data infrastructure
cd docker
docker-compose up -d  # PostgreSQL + Redis

# 5. Initialize feature store schema
python scripts/init_tables.py
python scripts/optimize_database.py  # Add performance indexes
python scripts/seed_sources.py

# 6. Run pipeline health check
python scripts/health_check.py

# 7. Test pipeline end-to-end
python scripts/integration_test.py
```

---

## ðŸ—ƒï¸ **Data Pipeline Operations**

### Incremental Daily Run
```bash
# Process new content from last 24 hours
python run_scrapers_with_pipeline.py --hours 24

# Monitor pipeline execution
python scripts/health_check.py

# Check feature store status
python -c "from app.queue import get_message_queue; print(get_message_queue().get_queue_stats())"
```

### Start Processing Workers
```bash
# Start async workers for feature generation
python scripts/run_workers.py

# Workers process articles through stages:
# 1. Content extraction
# 2. LLM summarization
# 3. Embedding generation
```

### Serve Features to Downstream Applications
```bash
# Add email subscription for batch delivery
python scripts/seed_subscription.py

# Generate and deliver feature digest
python scripts/send_digest_now.py --all --hours 24

# Features are formatted as HTML + email delivery
# Can be adapted for API serving or batch export
```

---

## ðŸ” **Data Quality & Observability**

### Pipeline Health Monitoring

```bash
# Comprehensive health check
python scripts/health_check.py
```

**Validates:**
- âœ… Database connectivity and schema
- âœ… pgvector extension enabled
- âœ… Redis cache availability
- âœ… Message queue status and depths
- âœ… Feature completeness metrics

### Data Lineage Tracking

Every article is tracked through the pipeline:

```sql
-- Query article processing status
SELECT 
    a.id,
    a.title,
    pq.status,
    pq.retry_count,
    s.summary IS NOT NULL as has_summary,
    e.embedding IS NOT NULL as has_embedding
FROM articles a
LEFT JOIN processing_queue pq ON a.id = pq.article_id
LEFT JOIN article_summaries s ON a.id = s.article_id  
LEFT JOIN article_embeddings e ON a.id = e.article_id
WHERE a.scraped_at >= NOW() - INTERVAL '24 hours';
```

### Performance Benchmarking

```bash
# Run performance benchmarks
python scripts/benchmark.py
```

**Measures:**
- Digest generation time (< 1s target)
- Email rendering time (< 0.5s target)
- Database query performance (< 100ms target)
- Theoretical pipeline capacity

---

## ðŸ“ **Project Structure**

```
AI-NEWS-AGGREGATOR/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ scrapers/          # Multi-source data ingestion (Bronze layer)
â”‚   â”‚   â”œâ”€â”€ youtube_scraper.py
â”‚   â”‚   â”œâ”€â”€ openai_scraper.py
â”‚   â”‚   â”œâ”€â”€ anthropic_scraper.py
â”‚   â”‚   â””â”€â”€ google_scraper.py
â”‚   â”œâ”€â”€ processing/        # Feature engineering pipeline (Silver â†’ Gold)
â”‚   â”‚   â”œâ”€â”€ llm_summarizer.py      # LLM-powered summarization
â”‚   â”‚   â”œâ”€â”€ content_extractor.py   # Content cleaning & validation
â”‚   â”‚   â””â”€â”€ embedding_generator.py # Vector embedding generation
â”‚   â”œâ”€â”€ orchestrator/      # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ workers.py             # RQ worker functions
â”‚   â”‚   â””â”€â”€ pipeline.py            # Pipeline coordination
â”‚   â”œâ”€â”€ email/             # Serving layer (batch delivery)
â”‚   â”‚   â”œâ”€â”€ digest_generator.py    # Feature aggregation
â”‚   â”‚   â”œâ”€â”€ renderer.py            # Output formatting
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”œâ”€â”€ database/          # Feature store schema
â”‚   â”‚   â””â”€â”€ models.py              # SQLAlchemy ORM  
â”‚   â”œâ”€â”€ queue/             # Async processing
â”‚   â””â”€â”€ cache/             # Redis caching layer
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_workers.py              # Start processing workers
â”‚   â”œâ”€â”€ health_check.py             # Pipeline monitoring
â”‚   â”œâ”€â”€ integration_test.py         # E2E validation
â”‚   â”œâ”€â”€ optimize_database.py        # Performance tuning
â”‚   â”œâ”€â”€ benchmark.py                # Performance benchmarks
â”‚   â””â”€â”€ send_digest_now.py          # Feature serving
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # PostgreSQL + Redis
â”œâ”€â”€ Dockerfile                      # Production container
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ pyproject.toml                  # Modern Python packaging
â””â”€â”€ README.md
```

---

## âš¡ **Performance Optimization**

This project implements production-grade optimizations:

**Database Performance:**
- **Connection Pooling:** SQLAlchemy engine configured with `pool_pre_ping=True` and `pool_recycle=3600`
- **Indexed Queries:** 10 strategic indexes on frequently queried columns (published_at, article_id, status)
- **Query Optimization:** Digest generation uses efficient JOINs with LIMIT clauses

**Batch Processing:**
- **Async Workers:** RQ processes jobs asynchronously with automatic retries
- **Queue Prioritization:** Separate queues for extraction, summarization, embedding

**Caching Strategy:**
- **Redis Cache:** API responses cached with TTL
- **Template Compilation:** Jinja2 templates compiled once on startup

**Optimization Script:**
```bash
# Add performance indexes (run once after DB setup)
python scripts/optimize_database.py
```

---

## ðŸ“¡ **Observability**

Production-ready monitoring and logging:

**Health Monitoring:**
```bash
python scripts/health_check.py
```

Validates:
- Database connectivity
- pgvector extension
- Redis cache
- Message queue status  
- Record counts per table

**Key Metrics Tracked:**
- Articles processed by status (pending/extracting/summarizing/embedding/complete/failed)
- Queue depths (extraction, summarization, embedding queues)
- Feature delivery logs
- Processing pipeline throughput

**State Tracking:**
- Every article tracked through pipeline stages in `processing_queue`
- Failed jobs automatically retry with exponential backoff (via RQ)
- Feature deliveries logged to `email_deliveries` table

---

## ðŸ§ª **Testing Strategy for Data Pipelines**

```bash
# 1. Health check - verify all components
python scripts/health_check.py

# 2. Integration tests - full pipeline validation
python scripts/integration_test.py

# 3. Performance benchmarks
python scripts/benchmark.py

# 4. Email serving test (downstream application)
python scripts/test_email.py --full-test --recipient your@email.com
```

**Test Coverage:**
- âœ… Database connectivity and schema validation
- âœ… Multi-source data ingestion
- âœ… LLM feature generation pipeline
- âœ… Vector embedding generation
- âœ… Feature serving (email delivery)
- âœ… Message queue processing
- âœ… Performance benchmarks

---

## ðŸ“ˆ **Production Deployment**

### Docker Deployment

**Build Container:**
```bash
docker build -t ai-news-aggregator:latest .
```

**Run Workers:**
```bash
docker run -d \
  --name news-workers \
  --restart=always \
  --env-file .env \
  --network host \
  ai-news-aggregator:latest \
  python scripts/run_workers.py
```

**Schedule Pipeline Jobs:**
```bash
# Add to crontab (crontab -e)

# Daily data ingestion at 6 AM
0 6 * * * docker exec news-workers python run_scrapers_with_pipeline.py --hours 24

# Daily feature delivery at 8 AM  
0 8 * * * docker exec news-workers python scripts/send_digest_now.py --all
```

### GitHub Actions (CI/CD)

```yaml
# .github/workflows/daily-pipeline.yml
name: Daily Feature Pipeline
on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - run: pip install uv && uv pip install .
      - run: python run_scrapers_with_pipeline.py --hours 24
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
```

---

## ðŸŽ¯ **Why This Matters for Data/AI Engineering Teams**

This project demonstrates critical capabilities for modern data organizations:

### 1. Production Feature Engineering
- Implements LLM-powered feature generation at scale
- Shows understanding of cost optimization and caching strategies
- Demonstrates how to operationalize cutting-edge AI for practical data products

### 2. Scalable Data Infrastructure
- Implements a functional feature store with vector search capabilities
- Shows data modeling for both transactional and vector similarity workloads
- Demonstrates pipeline patterns (idempotency, incremental processing, quality gates)

### 3. Production DataOps Practices
- Full observability with health checks and structured logging
- Data quality validation integrated into pipeline
- Deployment strategies for both development and production

### 4. Transferable Architecture Patterns
Every component of this pipeline maps directly to production data infrastructure:
- **Ingestion Layer** â†’ Event streaming or batch data ingestion
- **Feature Engineering** â†’ ML feature pipelines or data transformation
- **Feature Store** â†’ Production ML feature serving infrastructure
- **Monitoring** â†’ Data reliability engineering practices

---

## ðŸ¤ **Contributing**

Contributions welcome! This project demonstrates data engineering patterns. Focus areas:

- **New Data Sources:** Implement additional source connectors
- **Enhanced Data Quality:** Add new validation checks
- **Performance Optimizations:** Improve pipeline throughput
- **Monitoring Enhancements:** Add Prometheus metrics, Grafana dashboards

---

## ðŸ“„ **License**

MIT License - see LICENSE file

---

**Built with â¤ï¸ for production data pipelines**